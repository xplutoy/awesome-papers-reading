#+title:      Active Learning
#+date:       [2024-07-04 Thu 00:38]
#+filetags:   :paper:
#+identifier: 20240704T003809

* 2024

** [#A] [[https://proceedings.neurips.cc/paper/2007/hash/ccc0aa1b81bf81e16c676ddb977c5881-Abstract.html][Discriminative Batch Mode Active Learning]] :nips:2007:

论文思路概览：

- 半监督最小熵学习目标函数：

$$\sum_{i\in L}\log P(y_i|\mathbf{x}_i,\mathbf{w})+\alpha\sum_{j\in U}\sum_{y=\pm1}P(y|\mathbf{x}_j,\mathbf{w})\log P(y|\mathbf{x}_j,\mathbf{w})$$

- Score 函数：

$$f(S)=\sum_{i\in L^t\cup S}\log P(y_i|\mathbf{x}_i,\mathbf{w}^{t+1})-\alpha\sum_{j\in U^t\setminus S}H(y|\mathbf{x}_j,\mathbf{w}^{t+1})$$

- 近似 Score 函数：

$$f(S)=\max_{\mathbf{y}S}\sum_{i\in L^t\cup S}\log P(y_i|\mathbf{x}_i,\mathbf{w}^{t+1})-\alpha\sum_{j\in U^t\setminus S}H(y|\mathbf{x}_j,\mathbf{w}^{t+1})$$

这里是从 $\mathbf{y}S$ 的所有配置中，选取使 $f(S)$ 值最大的 $S$ 。

文中后面还把上述批次主动学习表述为明确的数学优化问题来解决，值得细究，这里暂且不提。

** [#A] [[https://arxiv.org/abs/1708.00489][Active Learning for Convolutional Neural Networks: A Core-Set Approach]] :iclr:2018:

经典的 uncertainty 的方法一次只筛选一个样本进行标记，扩展为 Batch Sample 时又会导致数据相关性数据相关性问题，无法很好的扩展到深度学习上来。针对这个问题，论文把 Batch Active Learning 转化成 Core-Set Selection 的问题来处理，可以避免批量采样导致的 correlations 问题：

$$\min_{\mathbf{s}^1:|\mathbf{s}^1|\leq b}\left|\frac1n\sum_{i\in[n]}l(\mathbf{x}_i,y_i;A_{\mathbf{s}^0\cup\mathbf{s}^1})-\frac1{|\mathbf{s}^0+\mathbf{s}^1|}\sum_{j\in\mathbf{s}^0\cup\mathbf{s}^1}l(\mathbf{x}_j,y_j;A_{\mathbf{s}^0\cup\mathbf{s}^1})\right|$$

论文中把主动学习归结为 Core-Set Selection 的部分很有意思，直观来讲，就是当前轮迭代中选出的点与之前选出来的点一起要构成所有点的一个 Core-Set 。 这需要访问所有点的标签，不可行，论文转而去优化它的一个上界，对这个上界的优化等价于一个 K-Center 问题，并不再依赖数据的标签。直观来讲，就是选择 b 个中心点，使数据点与其最近中心之间的最大距离最小化。

论文中数学比较多，有些地方自己还没读透，建议深读。

** [#B] [[https://arxiv.org/abs/1905.03677][Learning Loss for Active Learning]] :cvpr:2019:

用一个辅助网络来同时学习一个 loss 预测网络，然后用该预测损失在 active-learing loops 中来筛选样本进行标记。

[[./imgs/20240707174251_predicted_loss.png]]

两个网络进行联合训练，损失函数为： $L_{\mathrm{target}}(\hat{y},y)+\lambda\cdot L_{\mathrm{loss}}(\hat{l},l)$ ，其中损失预测模型的 loss 如下：

$$L_{\mathbf{loss}}(\hat{l^p},l^p)=\max\left(0,-\mathbb{I}(l_i,l_j)\cdot(\hat{l_i}-\hat{l_j})+\xi\right)$$

** [#A] [[https://arxiv.org/abs/1906.08158][BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning]] :nips:2019:

在这篇论文中，我第一次了解到 Batch Active Learning 的问题的重要性。

文中发现平凡的把 BALD 方法扩展到 Batch 场景中会选出冗余的样本，数据效率低下，因此提出 BatchBALD 方法：

[[file:./imgs/20240707143300_batch_bald.png]]

** [#B] [[https://arxiv.org/abs/2006.09916][Bayesian active learning for production, a systematic study and a reusable library]] :2020:

** [#B] The power of ensembles for active learning in image classification :cvpr:

论文中对 MC-Dropout 问题的一些讨论很有意思。

** [#B] [[https://arxiv.org/abs/1703.02910][Deep Bayesian Active Learning with Image Data]]

主动学习在高维数据空间中的应用仍然很困难。这篇文章把深度贝叶斯卷积网络应用到主动学习框架中，在高维的图像数据集上取得了不错的效果。

** [#A] [[https://openreview.net/forum?id=IVESH65r0Ar][A Simple Yet Powerful Deep Active Learning With Snapshots Ensembles]]

简单的方法，让人惊讶的效果，全面细致的写作，非常好的一篇关于深度主动学习的论文。

** [#B] Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design

** [#B] [[https://lilianweng.github.io/posts/2022-04-15-data-gen/][Learning with not Enough Data Part 3: Data Generation]]

** [#A] [[https://lilianweng.github.io/posts/2022-02-20-active-learning/][Learning with not Enough Data Part 2: Active Learning]]

非常全面的关于深度主动学习最近研究的总结博文。

** [#A] [[https://lilianweng.github.io/posts/2021-12-05-semi-supervised/][Learning with not Enough Data Part 1: Semi-Supervised Learning]]

非常全面的关于半监督学习最近研究的总结性博文。下面摘录一些有意思的知识点。

这些半监督学习中用到的一些假设：

- Smoothness Assumptions :: 如果特征空间相近的两个点，其对应的标签也相近。
- Cluster Assumptions :: 特征空间有稠密区和稀疏区，稠密区的点自然形成簇，同一簇的样本具有相同的标签。
- Low-density Separation Assumptions :: 类别之间的决策边界往往位于稀疏、低密度区域。
- Manifold Assumptions :: 高维数据往往位于低维流形上。

整篇文章介绍了四大类半监督方法：Consistency Regularization、Pseudo Labeling、Pseudo Labeling with Consistency Regularization 和 Combined with Powerful Pre-Training 。


** [#B] Efficient Construction Method for Phase Diagrams Using Uncertainty Sampling

不确定性采样（Uncertainty Sampling）是一种主动学习（Active Learning）策略，其核心思想是从未标记的数据集中选择模型最不确定的样本来进行人工标注。这种方法优先选择对模型改进最有帮助的样本进行标注，可以高效地使用标注资源，通常使用一些启发式的不确定性度量，比如概率最低的类别、预测概率差异最大的样本或者模型输出的熵最高的样本。

利用 uncertainty sampling (US) 来构建相图。 整个未确定点的相图可用 $p(p|x)$ 描述，其中 x 表示相图中未确定点的位置向量，p 是相的类别。相图就可以从这个分布中描绘出来。 下图是该方法的一个示意图：

#+attr_org: :width 900
[[file:imgs/2024-04-14_17-24-12_screenshot.png]]

- 初始化 :: 随机选择几个点，然后通过实验和仿真确定其相
- 相估计 :: 利用半监督学习方法 label propagation、label spreading 来估计整个相图分布 $p(p|x)$
- 不确定性评分 :: 利用前一步估计的分布 $p(p|x)$ ，对所有未确定的点做不确定评分
- 验证 :: 选取不确定评分高的点进行实验，并从返步骤二再次更新相图分布，进行下一步迭代

该文的创新在于用 Uncertainty Sampling 来进行不确定估计，文中用了三种不确定性采样策略。

** [#B] [[https://www.nature.com/articles/s41586-023-06734-w][An autonomous laboratory for the accelerated synthesis of novel materials]] :nature:

利用主动学习，自动化实验室来加速材料研发。

** [#B] [[https://www.nature.com/articles/s41524-019-0153-8][Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design]] :nature:



** [#A] [[https://arxiv.org/abs/2202.13753][Machine learning–enabled high-entropy alloy discovery]]

提出了一种主动学习策略在数据及其稀疏的情况下来加速材料的发现。 下图是整个方法的示意图：

#+attr_org: :width 900
[[file:imgs/2024-04-19_21-00-49_screenshot.png]]

其中包括主要的三个步骤：

- 定向成分生成 :: 用 WVAE 定向生成具有特定属性的合金成分，尽管只是用合金成分来训练 WVAE，但却会学到具有物理意义的隐层特征。然后通过 GMM 和 MCMC 采样去搜索大量的合金成分。
- 物理信息测试 :: 两阶段的集成回归，并用 ranking-based policy 选择最有潜力的成分进行下一步的高通实验验证
- 高通实验反馈 :: 验证实验得到的数据再次反馈到 Invar database 中进行下一轮实验迭代

论文代码：https://github.com/ziyuanrao11/Machine-learning-enabled-high-entropy-alloy-discovery

总结：一篇非常不错的用机器学习做材料发现的论文，结合了当前人工智能许多前沿技术，如主动学习，物理信息网络，生成模型等。
