title:      Awesome Papers Reading
#+date:       [2024-04-14 Sun 16:40]
#+filetags:   :note:

工作后，阅读文献的时间少了。记录一下，激励自己。

* 2024

** [#B] [[https://arxiv.org/abs/1906.02506][Practical Deep Learning with Bayesian Principles]]

这篇文章证实 naturalgradient forvariational inference(NGVI) 可以实际有效的用于训练深度神经网络。其主要贡献在于发现了 SGD 的梯度的更新公式与 NGVI 的更新公式很相似，然后可以把在深度神经网络的训练技巧，比如 batch normalisation（BN）、data augmentation (DA)、learning rate schedule、momentum and initialisation，迁移到 NGVI 的训练上来。这种 Variational Online Gauss-Newton (VOGN) 方法同时结合了 SGD 和 NGVI 各自的优点，在可扩展的基础上同时保留了贝叶斯原则的优点。

** [#B] [[https://arxiv.org/abs/2005.11401][Retrieval-Augmentved Generation for Knowledge-Intensive NLP Tasks]]

预训练模型在“知识密集型”任务上仍有很多限制，其性能仍落后于“task-specific”模型。针对该问题，这篇文章探索了一种用于检索增强生成（RAG）的通用微调方法。结构如下图： 

#+attr_org: :width 900px
[[./imgs/20240618164121_rag.png]]

整个模型概率表示如下：

\[p_{\text{RAG-Token}}(y|x) \approx \prod_i^N\sum_{z\in\text{top-}k(p(\cdot|x))}p_\eta(z|x)p_\theta(y_i|x,z,y_{1:i-1})\] 

由检索器 $p_{\eta}(z|x)$ 和 Seq-2-Seq 生成器 $p_{\theta}(y_{i}|x,z,y_{1:i-1})$ 构成。
