#+title: Awesome Papers Reading
#+date: [2024-04-14 Sun 16:40]
#+filetags: :note:
#+tags: nips iclr

工作后，阅读文献的时间少了。记录一下，激励自己。

* 2024

** [#B] [[https://arxiv.org/abs/2203.11171][Self-Consistency Improves Chain of Thought Reasoning in Language Models]] :iclr:

这篇文章在 CoT 的基础上发现复杂推理任务通常可以有多个推理路径，这些路径的推理都可以得到结果，但答案的正确性却有一定的自洽性（self-consistency）。因此只要让 LLM 多产生一些 CoT，然后从所有生成的 CoT 中选择“自洽性”最好的答案作为结果就可以显著提升模型的推理能力。

#+attr_org: :width 900px
[[file:imgs/20240621191628_sc_cot.png]]

** [#B] [[https://arxiv.org/abs/2201.11903][Chain-of-Thought Prompting Elicits Reasoning in Large Language Models]] :nips:

提出了一种 CoT 的提示词技巧，通过在提示词中加入推理的中间过程来挖掘 LLM 的潜力。这种方法在算术推理、常识推理和符号推理等任务上比现有方法都更优异。

** [#A] [[https://arxiv.org/abs/2210.03629][ReAct: Synergizing Reasoning and Acting in Language Models]] :iclr:

提出了一种基于提示词的新学习范式，协同语言模型中的推理和行动来解决一般任务。其基本思路是扩充智能代理（Agent）的动作空间（action space）： $\hat{\mathcal{A}}=\mathcal{A}\cup\mathcal{L}$ ，其中 $\mathcal{L}$ 是语言空间。语言空间中的一个动作 $\hat{a}_t\in\mathcal{L}$ 表示一步“推理”或者“思考”，因此不会影响外部环境，也不会触发外部观察的反馈（observation feedback）。它们的主要作用在于根据当前上下文 $C_{t}$ 来推理出有用的信息，并更新上下文 $c_{t+1}=(c_t,\hat{a}_t)$ 以支持下一步的动作或推理。

这篇文章又刷新了我对 LLM 能力的认知边界。用 LLM 作为推理引擎的决策智能体将会成为以后的趋势。

** [#B] [[https://arxiv.org/abs/1906.02506][Practical Deep Learning with Bayesian Principles]] :nips:

这篇文章证实 naturalgradient forvariational inference(NGVI) 可以实际有效的用于训练深度神经网络。其主要贡献在于发现了 SGD 和 NGVI 两者的梯度更新公式很相似，然后可以把在深度神经网络的优化技巧，比如 batch normalisation（BN）、data augmentation (DA)、learning rate schedule、momentum and initialisation，迁移到 NGVI 的训练上来。这种 Variational Online Gauss-Newton (VOGN) 方法在保证一定可扩展的基础上同时保留了贝叶斯原则的很多优点。

** [#B] [[https://arxiv.org/abs/2005.11401][Retrieval-Augmentved Generation for Knowledge-Intensive NLP Tasks]] :nips:

预训练模型在“知识密集型”任务上仍有很多限制，其性能仍落后于“task-specific”模型。针对该问题，这篇文章探索了一种用于检索增强生成（RAG）的通用微调方法。结构如下图：

#+attr_org: :width 900px
[[file:./imgs/20240618164121_rag.png]]

整个模型概率表示如下：

\[p_{\text{RAG-Token}}(y|x) \approx \prod_i^N\sum_{z\in\text{top-}k(p(\cdot|x))}p_\eta(z|x)p_\theta(y_i|x,z,y_{1:i-1})\]

由检索器 $p_{\eta}(z|x)$ 和 Seq-2-Seq 生成器 $p_{\theta}(y_{i}|x,z,y_{1:i-1})$ 构成。

** [#B] [[https://arxiv.org/abs/2005.14165][Language Models are Few-Shot Learners]] :nips:

1. 提出了 LLM 的 In-Context-Learning ，在 inference 阶段把在训练阶段学到的技能（模式识别等）应用于目标任务。
2. LLM 的 In-Context-Learning 能力随模型增大而增强

第一次听到 In-Context-Learning 时，感觉挺疑惑的，不用更新模型参数，只需提供一些样例，就可以让模型输出结果超越 Fine-Tuning 后的模型。

** [#B] [[https://arxiv.org/pdf/2203.15556][Training Compute-Optimal Large Language Models]] :nips:

给出了在预算 C 一定的情况下最优模型规模 N 和训练数据量 D 的近似扩展法则：

\[N_{\mathrm{opt}}\approx C^a,D_{\mathrm{opt}}\approx C^b\]

其首先通过控制变量实验来获得性能与模型规模以及训练数据量的关系：

\[L(N,D)=E+\frac A{N^\alpha}+\frac B{D^\beta}\]

然后在通过预算 C 的约束来优化上述目标，得出最优模型规模 N 和 训练数据量 D 与预算 C 的关系。 研究思路值得认真学习。

直观一点，训练 LLM 的最优 Token/Param 比为：20/1 。不过最近又有很多新的研究。



