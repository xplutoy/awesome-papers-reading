#+title:      Uncertainty
#+date:       [2024-07-03 Wed 23:52]
#+filetags:   :paper:
#+identifier: 20240703T235249

* 2024

** [#A] Generalized Out-of-Distribution Detection: A Survey :2024:

** [#A] Energy-based Out-of-distribution Detection :2020:nips:
** [#A] A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks :2018:nips:

[cite:@leeSimpleUnifiedFramework2018] introduce a confidence score, denoted as M(x), which is defined using the Mahalanobis distance between test sample x and the closest class-conditional Gaussian distribution.

$$M(\mathbf{x})=\max_c-\left(f(\mathbf{x})-\widehat{\mu}_c\right)^\top\widehat{\boldsymbol{\Sigma}}^{-1}\left(f(\mathbf{x})-\widehat{\mu}_c\right).$$

This confidence score is robust for Out-of-Distribution detection as well as adversarial samples detection.

** [#A] [[https://arxiv.org/abs/1703.04977][What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?]] :2017:nips:

文中把不确定性分为了两类：模型不确定性（epistemic uncertainty），数据不确定性（aleatoric uncertainty）。

模型不确定性一般把模型权重当作随机变量，用它的后验分布来捕获其不确定性；对于数据不确定性，可以把模型的输出当作一个分布的参数，同样用这个分布来捕获其不确定性。文中提出了一个深度贝叶斯学习框架来同时捕获这两种分布：

$$[\hat{\mathbf{y}},\hat{\sigma}^2]=\mathbf{f}^{\widehat{\mathbf{W}}}(\mathbf{x}),\widehat{\mathbf W}\sim q(\mathbf W)$$

另外，在不同的场景中，这两类不确定性有不同的重要性。比如在 Large data situations ，Real-time applications 场景中数据不确定性很重要；但在 Safety-critical applications ，Small datasets 场景中，模型知道自己“不知道什么”是很重要的，所以模型不确定性就更关键。

非常有启发性的一篇论文。

** [#B] [[https://arxiv.org/abs/2202.06985][Deep Ensembles Work, But Are They Necessary?]] :2020:nips:

本论文推翻了最近 Deep Ensembles 研究的两个认知或假设：

- Hypothesis: ensemble diversity is responsible for improved UQ.
- Hypothesis: ensemble diversity is responsible for improved robustness.

在实验中，如果控制单个模型的 uncertainty 和数据分布内性能（InD performance），集成后并不会获得比单个模型更好的 UQ/robustness 的提升。

** [#B] [[https://arxiv.org/abs/1902.02476][A Simple Baseline for Bayesian Uncertainty in Deep Learning]] :nips:2019:

本文提出用 SGD iterates 中的信息来构建网络权重的后验分布，其构建权重后验分布的思路很有意思。

有意思的发现：

#+begin_quote
A key geometric observation in this paper is that the posterior distribution over neural network parameters is close to Gaussian in the subspace spanned by the trajectory of SGD.
#+end_quote

** [#B] [[https://arxiv.org/abs/1912.02757][Deep Ensembles: A Loss Landscape Perspective]] :2020:

从性能曲面的角度分析了 Deep Ensembles 方法为什么好于当前的贝叶斯方法：随机初始化参数分别训练得到的模型的多样性比在一条优化路径上的贝叶斯后验采样得到的模型更好。论文的实验很丰富，也有意思。

** [#A] [[https://arxiv.org/abs/1806.01768][Evidential Deep Learning to Quantify Classification Uncertainty]] :nips:2018:

非常新颖的方法，涉及很多其他知识，如 Dempster–Shafer Theory of Evidence (DST) ，Subjective Logic (SL) 等。需要再读再理解。

** [#A] [[https://arxiv.org/abs/1711.09325][Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples]] :iclr:2018:

有意思的两点：

- CONFIDENT CLASSIFIER FOR OUT-OF-DISTRIBUTION

$$\min_\theta\mathbb{E}_{P_{\mathrm{in}}(\widehat{\mathbf{x}},\widehat{y})}\Big[-\log P_\theta\left(y=\widehat{y}|\widehat{\mathbf{x}}\right)\Big]+\beta\mathbb{E}_{P_{\mathrm{out}}(\mathbf{x})}\Big[KL\left(\mathcal{U}\left(y\right)\parallel P_\theta\left(y|\mathbf{x}\right)\right)\Big]$$

- 从 in-distribution 的周围采样 out-of-distribution 样本更有效，并提出一种 GAN 的方法来生成 out-of-distribution 样本

\begin{aligned}
\min_{G}\max_{D}&\beta\underbrace{\mathbb{E}_{P_G(\mathbf{x})}\big[KL\left(\mathcal{U}\left(y\right)\parallel P_\theta\left(y|\mathbf{x}\right)\right)\big]}_{(\mathbf{a})}\\&+\underbrace{\mathbb{E}_{P_{\mathrm{in}}(\mathbf{x})}\big[\log D\left(\mathbf{x}\right)\big]+\mathbb{E}_{P_G(\mathbf{x})}\big[\log\left(1-D\left(\mathbf{x}\right)\right)\big]}_{(\mathbf{b})}
\end{aligned}

值得细读。

** [#A] [[https://openreview.net/forum?id=H1VGkIxRZ][Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks]] :iclr:2018:

这篇论文在 [cite:@hendrycksBaselineDetectingMisclassified2022] 的基础上提出了新地改进。改进的思路主要基于如下发现： 通过适当的修改 softmax 的温度 T 和对输入添加适当的扰动：  $\tilde{\boldsymbol{x}}=\boldsymbol{x}-\varepsilon\mathrm{sign}(-\nabla_{\boldsymbol{x}}\log S_{\hat{y}}(\boldsymbol{x};T))$ 可以使 in- 和 out-of-distribution 的图像的 softmax-score 的分布更加分离。论文写作清晰，内容充实，值得多读。

** [#B] [[https://arxiv.org/abs/1610.02136][A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks]] :iclr:2017:

这篇文献发现了这样一个现象：虽然 softmax 输出经常产生过高置信度的预测，但错误样本或OoD样本的预测结果，仍然会倾向于给出相对于正确样本较低的预测概率。基于此，统计 softmax 输出就可以检测异常样本和错误分类样本。

** [#A] [[https://arxiv.org/abs/1802.10501][Predictive Uncertainty Estimation via Prior Networks]] :2018:nips:

在贝叶斯框架中，分布不确定性，即由于测试数据和训练数据的分布不匹配而产生的不确定性（OOD问题），被认为是模型不确定性的一部分。在这项工作中，分布不确定性被单独识别出来，用先验网络对其进行显式建模，并与数据不确定性和模型不确定性统一在一个可解释的概率框架下。

$$\mathsf{P}(\omega_c|\boldsymbol{x}^*,\mathcal{D})=\int\int\underbrace{\mathsf{p}(\omega_c|\boldsymbol{\mu})}_{Data}\underbrace{\mathsf{p}(\boldsymbol{\mu}|\boldsymbol{x}^*,\boldsymbol{\theta})}_{Distributional}\underbrace{\mathsf{p}(\boldsymbol{\theta}|\mathcal{D})}_{Model}d\boldsymbol{\mu}d\boldsymbol{\theta}$$

可以用 y 和 μ 之间的互信息来估计分布不确定性：

$$\underbrace{\mathcal{I}[y,\boldsymbol{\mu}|\boldsymbol{x}^*;\mathcal{D}]}_{Distributional\mathrm{~}Uncertainty}=\underbrace{\mathcal{H}[\mathbb{E}_{\mathbf{p}(\boldsymbol{\mu}|\boldsymbol{x}^*;\mathcal{D})}[\mathbb{P}(y|\boldsymbol{\mu})]]}_{Total\mathrm{~}Uncertainty}-\underbrace{\mathbb{E}_{\mathbf{p}(\boldsymbol{\mu}|\boldsymbol{x}^*;\mathcal{D})}[\mathcal{H}[\mathbb{P}(y|\boldsymbol{\mu})]]}_{Expected\mathrm{~}Data\mathrm{~}Uncertainty}$$

或者微分熵：

$$\mathcal{H}[\mathfrak{p}(\boldsymbol{\mu}|\boldsymbol{x}^*;\mathcal{D})]=-\int\mathfrak{p}(\boldsymbol{\mu}|\boldsymbol{x}^*;\mathcal{D})\ln(\mathfrak{p}(\boldsymbol{\mu}|\boldsymbol{x}^*;\mathcal{D}))d\boldsymbol{\mu}$$

这里的 $p(\boldsymbol{\mu}|x^{*};D)$ 是一个定义在单形上的分布。文中选用 Dirichlet 分布来表示，其是分类分布的共轭先验。

实验结果显示，DPN 的微分熵对 OOD 的检测最有效。

信息容量极大的文章，值得反复阅读。

** [#A] [[https://arxiv.org/abs/1807.00263][Accurate Uncertainties for Deep Learning Using Calibrated Regression]] :icml:2018:

在算法上理解了很久，对概率知识生疏了。关键在于理解校正的充分条件：

$$\frac{\sum_{t=1}^T\mathbb{I}\{y_t\leq F_t^{-1}(p)\}}T\to p\text{ for all }p\in[0,1]$$

结合 [[https://github.com/AnthonyRentsch/calibrated_regression/blob/master/FinalProjectReport.ipynb][Implementing Calibrated Regression]] 上的一张图来直观的说明：

#+attr_org: :width 900px
[[file:imgs/20240629105704_calibrate.png]]

也就是校验好的 Regression 的经验 CDF 与预测的 CDF 必须一致。

** [#A] [[https://arxiv.org/abs/1706.04599][On Calibration of Modern Neural Networks]] :icml:2017:

如果你准备了解这个领域，就从这篇文献开始吧。文中有对问题清晰的定义，同时还有很多有意思的发现和见解。

** [#B] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles

